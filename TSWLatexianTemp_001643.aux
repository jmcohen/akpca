\relax 
\citation{scholkopf1997}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\citation{scholkopf1997}
\@writefile{toc}{\contentsline {section}{\numberline {2}Principal Components Analysis}{2}}
\newlabel{pca-opt}{{1}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Centering}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Kernel PCA}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivating Example}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Original data in $\mathbb  {R}^2$ (left) and the same data encoded into $\mathbb  {R}$ using PCA (right).}}{3}}
\newlabel{redblue}{{1}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Data from figure \G@refundefinedtrue \text  {\normalfont  \bfseries  ??}\GenericWarning  {               }{LaTeX Warning: Reference `redblue' on page 3 undefined} mapped in $\mathbb  {R}^3$ ``feature space'' using the degree-2 polynomial map (left), and the projection of this transformed data onto the first principal component (right). }}{3}}
\newlabel{redblue-2}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}The Kernel Trick}{3}}
\citation{scholkopf2002}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Derivation}{4}}
\newlabel{ev-equation}{{4}{4}}
\newlabel{linear-combo}{{5}{4}}
\newlabel{the-equation}{{7}{5}}
\newlabel{replace-with-kernel}{{9}{5}}
\newlabel{kk}{{10}{5}}
\newlabel{eigenvalue-problem}{{11}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Normalization}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Procedure}{5}}
\citation{williams2001}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Centering}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Nystrom Approximation}{6}}
\citation{kwok2003}
\citation{mika1999}
\@writefile{toc}{\contentsline {section}{\numberline {4}Autoencoding Kernel PCA}{7}}
\citation{johnson2013}
\newlabel{bad-objective}{{15}{8}}
\newlabel{the-inequality}{{16}{8}}
\newlabel{akpca-objective}{{17}{8}}
\newlabel{akpca-objective-unraveled}{{18}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Optimization}{8}}
\newlabel{grad-a}{{19}{9}}
\newlabel{grad-b}{{20}{9}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Data-span Autoencoding KPCA}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Derivation}{9}}
\newlabel{def-a}{{21}{10}}
\newlabel{def-b}{{22}{10}}
\newlabel{ds-objective}{{23}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Optimization}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Efficiently computing the gradient}{11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.0.1}Reconstuction Error on Training Set}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Average reconstruction error on the training set plotted as a function of number of components. Top left: 14x14, short training set. Top right: 14x14, long training set. Bottom: 28x28, short training set.}}{13}}
\newlabel{recon-err-training}{{3}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.0.2}Reconstuction Error on Held-Out Set}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Average reconstruction error on the held-out set, plotted as a function of number of components. Top left: 14x14, short training set. Top right: 14x14, long training set. Bottom: 28x28, short training set.}}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Average reconstruction error on the training vs. held-out set. Left: short training set (300 examples per class). Right: long training set (3,500 examples per class).}}{14}}
\newlabel{training-vs-test}{{5}{14}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.0.3}Denoising: Gaussian Noise}{14}}
\citation{mika1999}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Comparing reconstruction error on held-out data for a model trained on a ``short'' training set (300 examples per class) vs. a ``long'' training set (1750 examples per class). Across all algorithms, the model trained on a longer dataset exhibited lower (better) reconstruction error on held-out data.}}{15}}
\newlabel{short-vs-long}{{6}{15}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.0.4}Denoising: Speckle Noise}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Three MNIST digits corrupted by increasing levels of Gaussian noise. From left to right, the standard devaiation of the noise is: $\{0, 0.02, 0.04, 0.06\}$. }}{16}}
\newlabel{gaussian-noise}{{7}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Denoising error of all four algorithms in denoising MNIST digits that have been corrupted by Gaussian noise, with 10 (left) and 20 (right) components in the model.}}{17}}
\newlabel{gaussian-denoising}{{8}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Three MNIST digits corrupted by increasing levels of speckle noise. From left to right: $p$, the probability that a pixel is set to zero, is varied in $\{0, 0.2, 0.3, 0.4\}$.}}{18}}
\newlabel{speckle-noise}{{9}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Denoising error of all four algorithms in denoising MNIST digits that have been corrupted with speckle noise, as a function of $p$, the probability that a pixel is set to zero. Plots shown for models of 10 (left) and 20 (right) components.}}{19}}
\newlabel{speckle-denoising}{{10}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.0.5}Classification}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Cross-validated classification accuracy as a function of number of components, for small (left) and large (right) classification training set sizes.}}{21}}
\newlabel{classification-accuracy}{{6.0.5}{21}}
\bibcite{kwok2003}{1}
\bibcite{johnson2013}{2}
\bibcite{mika1999}{3}
\bibcite{scholkopf2002}{4}
\bibcite{scholkopf1997}{5}
\bibcite{williams2001}{6}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{22}}
